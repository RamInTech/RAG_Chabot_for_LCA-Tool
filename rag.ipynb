{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot with PDF Knowledge Base\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) chatbot. The chatbot will use the information from an uploaded PDF file as its knowledge base. We will use Hugging Face for models and FAISS for efficient document retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Dependencies\n",
    "\n",
    "First, we need to install the necessary Python libraries. We'll use:\n",
    "- `transformers` and `torch` for loading Hugging Face models.\n",
    "- `sentence-transformers` for creating embeddings.\n",
    "- `pypdf` to read and extract text from the PDF file.\n",
    "- `faiss-cpu` for creating the vector index for fast retrieval.\n",
    "- `datasets` to handle our text data easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Libraries\n",
    "\n",
    "Now, let's import all the required libraries for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ram/Downloads/lca_tool/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from pypdf import PdfReader\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load and Process the PDF\n",
    "\n",
    "We'll load the `Aluminium.pdf` file, extract its text content, and then split the text into smaller, manageable chunks. This chunking is important because language models have a limited context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and split the PDF into 62 chunks.\n",
      "\n",
      "--- Example Chunk ---\n",
      "Aluminium\n",
      "Ore & Mining: Bauxite ore (mainly in tropical countries) is the principal source of alumina. Global\n",
      "bauxite mines are often large open-pit operations producing 3–5 tonnes of ore per tonne of Al.\n",
      "(India’s bauxite reserves lie mainly in Odisha and Jharkhand.). \n",
      "Production Steps: Primary Al production is a three-step process (bauxite mining, alumina\n",
      "refining via Bayer , then Hall–Héroult electrolysis). In India, alumina (Al₂O₃) is refined (Bayer\n",
      "process) at plants in Odisha/Chhattisgarh, \n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=500, chunk_overlap=200):\n",
    "    \"\"\"Splits text into overlapping chunks.\"\"\"\n",
    "    wrapper = textwrap.TextWrapper(width=chunk_size, break_long_words=False, replace_whitespace=False)\n",
    "    chunks = []\n",
    "    current_pos = 0\n",
    "    while current_pos < len(text):\n",
    "        end_pos = current_pos + chunk_size\n",
    "        chunk = text[current_pos:end_pos]\n",
    "        chunks.append(chunk)\n",
    "        current_pos += chunk_size - chunk_overlap\n",
    "    return [chunk for chunk in chunks if chunk.strip()] # Remove empty chunks\n",
    "\n",
    "# Specify the path to your PDF file\n",
    "pdf_path = 'Aluminium.pdf'\n",
    "\n",
    "# Extract and chunk the text\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "text_chunks = split_text_into_chunks(pdf_text)\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "documents = {'text': text_chunks}\n",
    "dataset = Dataset.from_dict(documents)\n",
    "\n",
    "print(f\"Successfully loaded and split the PDF into {len(dataset)} chunks.\")\n",
    "print(\"\\n--- Example Chunk ---\")\n",
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Text Embeddings\n",
    "\n",
    "Next, we'll convert our text chunks into numerical vectors (embeddings) using a pre-trained model from Hugging Face. These embeddings capture the semantic meaning of the text, allowing us to find similar chunks based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created and added to the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "# This can take a few moments depending on the number of chunks\n",
    "embeddings = embedding_model.encode(dataset['text'], show_progress_bar=True)\n",
    "\n",
    "# Add the embeddings to our dataset\n",
    "dataset = dataset.add_column('embeddings', embeddings.tolist())\n",
    "\n",
    "print(\"Embeddings created and added to the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build the FAISS Index\n",
    "\n",
    "With our embeddings ready, we can create a FAISS index. FAISS (Facebook AI Similarity Search) is a library that allows for efficient searching of similar vectors. This index will function as our fast, searchable knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 62 vectors.\n"
     ]
    }
   ],
   "source": [
    "# Convert embeddings to a numpy array\n",
    "embeddings_np = np.array(dataset['embeddings'], dtype='float32')\n",
    "\n",
    "# Get the dimension of the embeddings\n",
    "d = embeddings_np.shape[1]\n",
    "\n",
    "# Create the FAISS index\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define the RAG Chatbot Logic\n",
    "\n",
    "This is the core of our chatbot. We'll define two main functions:\n",
    "1.  `retrieve_context`: This function takes a user's question, embeds it, and uses the FAISS index to find the most relevant text chunks from our PDF.\n",
    "2.  `generate_answer`: This function takes the user's question and the retrieved context and feeds them to a question-answering model to generate a final, coherent answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 11397.57it/s]\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Load a question-answering model\n",
    "qa_model_name = 'deepset/roberta-base-squad2'\n",
    "qa_pipeline = pipeline('question-answering', model=qa_model_name, tokenizer=qa_model_name)\n",
    "\n",
    "def retrieve_context(query, k=3):\n",
    "    \"\"\"Retrieves the top-k most relevant text chunks for a query.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    query_embedding_np = np.array(query_embedding, dtype='float32')\n",
    "    \n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(query_embedding_np, k)\n",
    "    \n",
    "    # Get the corresponding text chunks\n",
    "    retrieved_chunks = [dataset[i]['text'] for i in indices[0]]\n",
    "    return \" \".join(retrieved_chunks)\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    \"\"\"Generates an answer based on the query and retrieved context.\"\"\"\n",
    "    qa_input = {\n",
    "        'question': query,\n",
    "        'context': context\n",
    "    }\n",
    "    result = qa_pipeline(qa_input)\n",
    "    return result['answer']\n",
    "\n",
    "def chatbot(query):\n",
    "    \"\"\"The main chatbot function.\"\"\"\n",
    "    print(f\"❓ Query: {query}\")\n",
    "    \n",
    "    # 1. Retrieve context\n",
    "    context = retrieve_context(query)\n",
    "    # print(f\"\\n🔍 Retrieved Context:\\n{context}\") # Uncomment for debugging\n",
    "    \n",
    "    # 2. Generate answer\n",
    "    answer = generate_answer(query, context)\n",
    "    print(f\"\\n🤖 Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ask a Question!\n",
    "\n",
    "Now it's time to test our RAG chatbot. Let's ask a question based on the content of the `Aluminium.pdf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ Query: What are the main production stages of Aluminium?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Wrong key type: '45' of type '<class 'numpy.int64'>'. Expected one of int, slice, range, str or Iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m      2\u001b[39m user_query = \u001b[33m\"\u001b[39m\u001b[33mWhat are the main production stages of Aluminium?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mchatbot\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mchatbot\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m❓ Query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 1. Retrieve context\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m context = \u001b[43mretrieve_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# print(f\"\\n🔍 Retrieved Context:\\n{context}\") # Uncomment for debugging\u001b[39;00m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# 2. Generate answer\u001b[39;00m\n\u001b[32m     35\u001b[39m answer = generate_answer(query, context)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mretrieve_context\u001b[39m\u001b[34m(query, k)\u001b[39m\n\u001b[32m     11\u001b[39m distances, indices = index.search(query_embedding_np, k)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Get the corresponding text chunks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m retrieved_chunks = [\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[32m0\u001b[39m]]\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(retrieved_chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/lca_tool/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:2859\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2857\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2858\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[32m-> \u001b[39m\u001b[32m2859\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/lca_tool/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py:2841\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2839\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2840\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2841\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2844\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/lca_tool/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:654\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    653\u001b[39m     pa_table = table\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m query_type = \u001b[43mkey_to_query_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/lca_tool/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:574\u001b[39m, in \u001b[36mkey_to_query_type\u001b[39m\u001b[34m(key)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (\u001b[38;5;28mslice\u001b[39m, \u001b[38;5;28mrange\u001b[39m, Iterable)):\n\u001b[32m    573\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[43m_raise_bad_key_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/lca_tool/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py:44\u001b[39m, in \u001b[36m_raise_bad_key_type\u001b[39m\u001b[34m(key)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_raise_bad_key_type\u001b[39m(key: Any):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m     45\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWrong key type: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m of type \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Expected one of int, slice, range, str or Iterable.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: Wrong key type: '45' of type '<class 'numpy.int64'>'. Expected one of int, slice, range, str or Iterable."
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_query = \"What are the main production stages of Aluminium?\"\n",
    "chatbot(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_2 = \"How much energy does recycling aluminum save?\"\n",
    "chatbot(user_query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_3 = \"What is red mud?\"\n",
    "chatbot(user_query_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
