{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot with PDF Knowledge Base\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) chatbot. The chatbot will use the information from an uploaded PDF file as its knowledge base. We will use Hugging Face for models and ChromaDB for the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Dependencies\n",
    "\n",
    "First, we need to install the necessary Python libraries. We'll use:\n",
    "- `transformers` and `torch` for loading Hugging Face models.\n",
    "- `sentence-transformers` for creating embeddings.\n",
    "- `pypdf` to read and extract text from the PDF file.\n",
    "- `chromadb` for our vector database.\n",
    "- `datasets` to handle our text data easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch sentence-transformers pypdf chromadb datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Libraries\n",
    "\n",
    "Now, let's import all the required libraries for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from pypdf import PdfReader\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load and Process the PDF\n",
    "\n",
    "We'll load the `Aluminium.pdf` file, extract its text content, and then split the text into smaller, manageable chunks. This chunking is important because language models have a limited context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"Splits text into overlapping chunks.\"\"\"\n",
    "    wrapper = textwrap.TextWrapper(width=chunk_size, break_long_words=False, replace_whitespace=False)\n",
    "    chunks = []\n",
    "    current_pos = 0\n",
    "    while current_pos < len(text):\n",
    "        end_pos = current_pos + chunk_size\n",
    "        chunk = text[current_pos:end_pos]\n",
    "        chunks.append(chunk)\n",
    "        current_pos += chunk_size - chunk_overlap\n",
    "    return [chunk for chunk in chunks if chunk.strip()] # Remove empty chunks\n",
    "\n",
    "# Specify the path to your PDF file\n",
    "pdf_path = 'Aluminium.pdf'\n",
    "\n",
    "# Extract and chunk the text\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "text_chunks = split_text_into_chunks(pdf_text)\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "documents = {'text': text_chunks}\n",
    "dataset = Dataset.from_dict(documents)\n",
    "\n",
    "print(f\"Successfully loaded and split the PDF into {len(dataset)} chunks.\")\n",
    "print(\"\\n--- Example Chunk ---\")\n",
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Text Embeddings\n",
    "\n",
    "Next, we'll convert our text chunks into numerical vectors (embeddings) using a pre-trained model from Hugging Face. These embeddings capture the semantic meaning of the text, allowing us to find similar chunks based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "# This can take a few moments depending on the number of chunks\n",
    "embeddings = embedding_model.encode(dataset['text'], show_progress_bar=True)\n",
    "\n",
    "# Add the embeddings to our dataset\n",
    "dataset = dataset.add_column('embeddings', embeddings.tolist())\n",
    "\n",
    "print(\"Embeddings created and added to the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build the ChromaDB Collection\n",
    "\n",
    "With our embeddings ready, we can create a ChromaDB collection. This collection will store our vectors and allow for efficient similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChromaDB client (this will be an in-memory instance)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create a new collection or get it if it already exists\n",
    "collection = client.get_or_create_collection(name=\"aluminium_kb\")\n",
    "\n",
    "# ChromaDB requires string IDs for each document\n",
    "doc_ids = [str(i) for i in range(len(dataset))]\n",
    "\n",
    "# Add the documents and their embeddings to the collection\n",
    "collection.add(\n",
    "    embeddings=np.array(dataset['embeddings']),\n",
    "    documents=dataset['text'],\n",
    "    ids=doc_ids\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB collection created with {collection.count()} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define the RAG Chatbot Logic\n",
    "\n",
    "This is the core of our chatbot. We'll define two main functions:\n",
    "1.  `retrieve_context`: This function takes a user's question, embeds it, and uses the ChromaDB collection to find the most relevant text chunks from our PDF.\n",
    "2.  `generate_answer`: This function takes the user's question and the retrieved context and feeds them to a question-answering model to generate a final, coherent answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a question-answering model\n",
    "qa_model_name = 'deepset/roberta-base-squad2'\n",
    "qa_pipeline = pipeline('question-answering', model=qa_model_name, tokenizer=qa_model_name)\n",
    "\n",
    "def retrieve_context(query, k=3):\n",
    "    \"\"\"Retrieves the top-k most relevant text chunks for a query from ChromaDB.\"\"\"\n",
    "    # Convert the query to a list of embeddings\n",
    "    query_embedding = embedding_model.encode([query]).tolist()\n",
    "    \n",
    "    # Query the ChromaDB collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=k\n",
    "    )\n",
    "    \n",
    "    # The retrieved documents are in the 'documents' key\n",
    "    retrieved_chunks = results['documents'][0]\n",
    "    return \" \".join(retrieved_chunks)\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    \"\"\"Generates an answer based on the query and retrieved context.\"\"\"\n",
    "    qa_input = {\n",
    "        'question': query,\n",
    "        'context': context\n",
    "    }\n",
    "    result = qa_pipeline(qa_input)\n",
    "    return result['answer']\n",
    "\n",
    "def chatbot(query):\n",
    "    \"\"\"The main chatbot function.\"\"\"\n",
    "    print(f\"❓ Query: {query}\")\n",
    "    \n",
    "    # 1. Retrieve context\n",
    "    context = retrieve_context(query)\n",
    "    # print(f\"\\n🔍 Retrieved Context:\\n{context}\") # Uncomment for debugging\n",
    "    \n",
    "    # 2. Generate answer\n",
    "    answer = generate_answer(query, context)\n",
    "    print(f\"\\n🤖 Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ask a Question!\n",
    "\n",
    "Now it's time to test our RAG chatbot. Let's ask a question based on the content of the `Aluminium.pdf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "user_query = \"What are the main production stages of Aluminium?\"\n",
    "chatbot(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_2 = \"How much energy does recycling aluminum save?\"\n",
    "chatbot(user_query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_3 = \"What is red mud?\"\n",
    "chatbot(user_query_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
