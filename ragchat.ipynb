{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot with PDF Knowledge Base\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) chatbot. The chatbot will use the information from an uploaded PDF file as its knowledge base. We will use Hugging Face for models and FAISS for efficient document retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Dependencies\n",
    "\n",
    "First, we need to install the necessary Python libraries. We'll use:\n",
    "- `transformers` and `torch` for loading Hugging Face models.\n",
    "- `sentence-transformers` for creating embeddings.\n",
    "- `pypdf` to read and extract text from the PDF file.\n",
    "- `faiss-cpu` for creating the vector index for fast retrieval.\n",
    "- `datasets` to handle our text data easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Libraries\n",
    "\n",
    "Now, let's import all the required libraries for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ram/Downloads/lca_tool/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from pypdf import PdfReader\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load and Process the PDF\n",
    "\n",
    "We'll load the `Aluminium.pdf` file, extract its text content, and then split the text into smaller, manageable chunks. This chunking is important because language models have a limited context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and split the PDF into 42 chunks.\n",
      "\n",
      "--- Example Chunk ---\n",
      "Aluminium\n",
      "Ore & Mining: Bauxite ore (mainly in tropical countries) is the principal source of alumina. Global\n",
      "bauxite mines are often large open-pit operations producing 3â€“5â€¯tonnes of ore per tonne of Al.\n",
      "(Indiaâ€™s bauxite reserves lie mainly in Odisha and Jharkhand.). \n",
      "Production Steps: Primary Al production is a three-step process (bauxite mining, alumina\n",
      "refining via Bayer , then Hallâ€“HÃ©roult electrolysis). In India, alumina (Alâ‚‚Oâ‚ƒ) is refined (Bayer\n",
      "process) at plants in Odisha/Chhattisgarh, \n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"Splits text into overlapping chunks.\"\"\"\n",
    "    wrapper = textwrap.TextWrapper(width=chunk_size, break_long_words=False, replace_whitespace=False)\n",
    "    chunks = []\n",
    "    current_pos = 0\n",
    "    while current_pos < len(text):\n",
    "        end_pos = current_pos + chunk_size\n",
    "        chunk = text[current_pos:end_pos]\n",
    "        chunks.append(chunk)\n",
    "        current_pos += chunk_size - chunk_overlap\n",
    "    return [chunk for chunk in chunks if chunk.strip()] # Remove empty chunks\n",
    "\n",
    "# Specify the path to your PDF file\n",
    "pdf_path = 'Aluminium.pdf'\n",
    "\n",
    "# Extract and chunk the text\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "text_chunks = split_text_into_chunks(pdf_text)\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "documents = {'text': text_chunks}\n",
    "dataset = Dataset.from_dict(documents)\n",
    "\n",
    "print(f\"Successfully loaded and split the PDF into {len(dataset)} chunks.\")\n",
    "print(\"\\n--- Example Chunk ---\")\n",
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Text Embeddings\n",
    "\n",
    "Next, we'll convert our text chunks into numerical vectors (embeddings) using a pre-trained model from Hugging Face. These embeddings capture the semantic meaning of the text, allowing us to find similar chunks based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created and added to the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "# This can take a few moments depending on the number of chunks\n",
    "embeddings = embedding_model.encode(dataset['text'], show_progress_bar=True)\n",
    "\n",
    "# Add the embeddings to our dataset\n",
    "dataset = dataset.add_column('embeddings', embeddings.tolist())\n",
    "\n",
    "print(\"Embeddings created and added to the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build the FAISS Index\n",
    "\n",
    "With our embeddings ready, we can create a FAISS index. FAISS (Facebook AI Similarity Search) is a library that allows for efficient searching of similar vectors. This index will function as our fast, searchable knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 42 vectors.\n"
     ]
    }
   ],
   "source": [
    "# Convert embeddings to a numpy array\n",
    "embeddings_np = np.array(dataset['embeddings'], dtype='float32')\n",
    "\n",
    "# Get the dimension of the embeddings\n",
    "d = embeddings_np.shape[1]\n",
    "\n",
    "# Create the FAISS index\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings_np)\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define the RAG Chatbot Logic\n",
    "\n",
    "This is the core of our chatbot. We'll define two main functions:\n",
    "1.  `retrieve_context`: This function takes a user's question, embeds it, and uses the FAISS index to find the most relevant text chunks from our PDF.\n",
    "2.  `generate_answer`: This function takes the user's question and the retrieved context and feeds them to a question-answering model to generate a final, coherent answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13888.42it/s]\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Load a question-answering model\n",
    "qa_model_name = 'deepset/roberta-base-squad2'\n",
    "qa_pipeline = pipeline('question-answering', model=qa_model_name, tokenizer=qa_model_name)\n",
    "\n",
    "def retrieve_context(query, k=3):\n",
    "    \"\"\"Retrieves the top-k most relevant text chunks for a query.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    query_embedding_np = np.array(query_embedding, dtype='float32')\n",
    "    \n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(query_embedding_np, k)\n",
    "    \n",
    "    # Get the corresponding text chunks\n",
    "    # Cast numpy.int64 to int for dataset indexing\n",
    "    retrieved_chunks = [dataset[int(i)]['text'] for i in indices[0]]\n",
    "    return \" \".join(retrieved_chunks)\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    \"\"\"Generates an answer based on the query and retrieved context.\"\"\"\n",
    "    qa_input = {\n",
    "        'question': query,\n",
    "        'context': context\n",
    "    }\n",
    "    result = qa_pipeline(qa_input)\n",
    "    return result['answer']\n",
    "\n",
    "def chatbot(query):\n",
    "    \"\"\"The main chatbot function.\"\"\"\n",
    "    print(f\"â“ Query: {query}\")\n",
    "    \n",
    "    # 1. Retrieve context\n",
    "    context = retrieve_context(query)\n",
    "    # print(f\"\\nðŸ” Retrieved Context:\\n{context}\") # Uncomment for debugging\n",
    "    \n",
    "    # 2. Generate answer\n",
    "    answer = generate_answer(query, context)\n",
    "    print(f\"\\nðŸ¤– Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ask a Question!\n",
    "\n",
    "Now it's time to test our RAG chatbot. Let's ask a question based on the content of the `Aluminium.pdf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Query: What are the main production stages of Aluminium?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ram/Downloads/lca_tool/.venv/lib/python3.13/site-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Answer: \n",
      "bauxite mining, alumina refining, and electrolysis\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What are the main production stages of Aluminium?\"\n",
    "chatbot(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Query: How much energy does recycling aluminum save?\n",
      "\n",
      "ðŸ¤– Answer: ~95%\n"
     ]
    }
   ],
   "source": [
    "user_query = \"How much energy does recycling aluminum save?\"\n",
    "chatbot(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Query: What is red mud?\n",
      "\n",
      "ðŸ¤– Answer: bauxite  residue\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What is red mud?\"\n",
    "chatbot(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
