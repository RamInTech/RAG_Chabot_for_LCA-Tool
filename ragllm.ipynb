{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot with PDF Knowledge Base\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) chatbot. The chatbot will use the information from an uploaded PDF file as its knowledge base. We will use Hugging Face for models and ChromaDB for the vector store. \n",
    "\n",
    "This version uses a **generative LLM (`google/flan-t5-base`)** to provide comprehensive, natural-language answers instead of just extracting text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Dependencies\n",
    "\n",
    "First, we need to install the necessary Python libraries. We'll use:\n",
    "- `transformers` and `torch` for loading Hugging Face models.\n",
    "- `sentence-transformers` for creating embeddings.\n",
    "- `pypdf` to read and extract text from the PDF file.\n",
    "- `chromadb` for our vector database.\n",
    "- `datasets` to handle our text data easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch sentence-transformers pypdf chromadb datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Libraries\n",
    "\n",
    "Now, let's import all the required libraries for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from pypdf import PdfReader\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load and Process the PDF\n",
    "\n",
    "We'll load the `Aluminium.pdf` file, extract its text content, and then split the text into smaller, manageable chunks. This chunking is important because language models have a limited context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and split the PDF into 42 chunks.\n",
      "\n",
      "--- Example Chunk ---\n",
      "Aluminium\n",
      "Ore & Mining: Bauxite ore (mainly in tropical countries) is the principal source of alumina. Global\n",
      "bauxite mines are often large open-pit operations producing 3–5 tonnes of ore per tonne of Al.\n",
      "(India’s bauxite reserves lie mainly in Odisha and Jharkhand.). \n",
      "Production Steps: Primary Al production is a three-step process (bauxite mining, alumina\n",
      "refining via Bayer , then Hall–Héroult electrolysis). In India, alumina (Al₂O₃) is refined (Bayer\n",
      "process) at plants in Odisha/Chhattisgarh, \n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"Splits text into overlapping chunks.\"\"\"\n",
    "    chunks = []\n",
    "    current_pos = 0\n",
    "    while current_pos < len(text):\n",
    "        end_pos = current_pos + chunk_size\n",
    "        chunk = text[current_pos:end_pos]\n",
    "        chunks.append(chunk)\n",
    "        current_pos += chunk_size - chunk_overlap\n",
    "    return [chunk for chunk in chunks if chunk.strip()] # Remove empty chunks\n",
    "\n",
    "# Specify the path to your PDF file\n",
    "pdf_path = 'Aluminium.pdf'\n",
    "\n",
    "# Extract and chunk the text\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n",
    "text_chunks = split_text_into_chunks(pdf_text)\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "documents_dict = {'text': text_chunks}\n",
    "dataset = Dataset.from_dict(documents_dict)\n",
    "\n",
    "print(f\"Successfully loaded and split the PDF into {len(dataset)} chunks.\")\n",
    "print(\"\\n--- Example Chunk ---\")\n",
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Text Embeddings\n",
    "\n",
    "Next, we'll convert our text chunks into numerical vectors (embeddings) using a pre-trained model from Hugging Face. These embeddings capture the semantic meaning of the text, allowing us to find similar chunks based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created and added to the dataset.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "# This can take a few moments depending on the number of chunks\n",
    "embeddings = embedding_model.encode(dataset['text'], show_progress_bar=True)\n",
    "\n",
    "# Add the embeddings to our dataset\n",
    "dataset = dataset.add_column('embeddings', embeddings.tolist())\n",
    "\n",
    "print(\"Embeddings created and added to the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build the ChromaDB Collection\n",
    "\n",
    "With our embeddings ready, we can create a ChromaDB collection. This collection will store our vectors and allow for efficient similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB collection created with 42 documents.\n"
     ]
    }
   ],
   "source": [
    "# Create a ChromaDB client (this will be an in-memory instance)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create a new collection or get it if it already exists\n",
    "collection = client.get_or_create_collection(name=\"aluminium_kb\")\n",
    "\n",
    "# ChromaDB requires string IDs for each document\n",
    "doc_ids = [str(i) for i in range(len(dataset))]\n",
    "\n",
    "# **FIX:** Explicitly convert the documents column to a standard Python list\n",
    "documents_list = [doc for doc in dataset['text']]\n",
    "\n",
    "# Add the documents and their embeddings to the collection\n",
    "collection.add(\n",
    "    embeddings=np.array(dataset['embeddings']),\n",
    "    documents=documents_list, # Use the explicit list\n",
    "    ids=doc_ids\n",
    ")\n",
    "\n",
    "print(f\"ChromaDB collection created with {collection.count()} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define the RAG Chatbot Logic with a Generative LLM\n",
    "\n",
    "This is the core of our chatbot. We'll define two main functions:\n",
    "1.  `retrieve_context`: This function takes a user's question, embeds it, and uses the ChromaDB collection to find the most relevant text chunks from our PDF.\n",
    "2.  `generate_answer`: This function now uses a **generative LLM (`google/flan-t5-base`)**. It takes the user's question and the retrieved context, and *generates* a new, human-like answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Load a generative text-to-text model (Flan-T5 Large for better generation)\n",
    "llm_model_name = 'google/flan-t5-large'\n",
    "llm_pipeline = pipeline('text2text-generation', model=llm_model_name, tokenizer=llm_model_name)\n",
    "\n",
    "def retrieve_context(query, k=3):\n",
    "    \"\"\"Retrieves the top-k most relevant text chunks for a query from ChromaDB.\"\"\"\n",
    "    # Convert the query to a list of embeddings\n",
    "    query_embedding = embedding_model.encode([query]).tolist()\n",
    "    \n",
    "    # Query the ChromaDB collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=k\n",
    "    )\n",
    "    \n",
    "    # The retrieved documents are in the 'documents' key\n",
    "    retrieved_chunks = results['documents'][0]\n",
    "    return \" \".join(retrieved_chunks)\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    \"\"\"Generates a natural language answer based on the query and retrieved context.\"\"\"\n",
    "    # Create a detailed prompt for the generative model\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Based on the following context, provide a comprehensive, detailed, and natural-sounding answer to the question. Explain your answer step by step if necessary.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate the answer using the LLM pipeline with parameters for longer, more varied responses\n",
    "    result = llm_pipeline(prompt, max_length=512, min_length=50, do_sample=True, temperature=0.7, top_p=0.9, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    # Extract the generated text\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "def chatbot(query):\n",
    "    \"\"\"The main chatbot function.\"\"\"\n",
    "    print(f\"❓ Query: {query}\")\n",
    "    \n",
    "    # 1. Retrieve context\n",
    "    context = retrieve_context(query)\n",
    "    # print(f\"\\n🔍 Retrieved Context:\\n{context}\") # Uncomment for debugging\n",
    "    \n",
    "    # 2. Generate answer\n",
    "    answer = generate_answer(query, context)\n",
    "    print(f\"\\n🤖 Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ask a Question!\n",
    "\n",
    "Now it's time to test our new and improved RAG chatbot. The answers should be much more detailed and conversational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ Query: What are the main production stages of Aluminium?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Answer: bauxite mining, alumina refining, and electrolysis\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What are the main production stages of Aluminium?\"\n",
    "chatbot(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ Query: How much energy does recycling aluminum save?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Answer: 95%\n"
     ]
    }
   ],
   "source": [
    "user_query_2 = \"How much energy does recycling aluminum save?\"\n",
    "chatbot(user_query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ Query: What is red mud?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=256) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Answer: Bauxite residue\n"
     ]
    }
   ],
   "source": [
    "user_query_3 = \"What is red mud?\"\n",
    "chatbot(user_query_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (584 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ Query: What are the environmental impacts of aluminum production?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Answer: Environmental impacts include GHGs (CO2, PFCs) and fluorides. Recycling rates: India 30–35%, global 65–75%. Typical waste: red mud (1–2 t per t alumina) and spent pot linings. Product lifetimes vary: packaging (years), transport/ construction (decades).\",\"metadata\":\"title\":\"Aluminium Production Facts\",\"author\":\"SynthKB\",\"date\":\"2025-09-01\",\"jurisdiction\":\"global/ India\",\"metadata\":\"title\":\"Aluminium Production Facts\",\"author\":\"SynthKB\",\"date\":\"2025-09-01\",\"jurisdiction\":\"global/ India\",\"metadata\":\"title\":\"Aluminium Production Facts\",\"author\":\"SynthKB\",\"date\":\"2025-09-01\",\"jurisdiction\":\"global/ India\",\"metadata\":\"title\":\"A\n"
     ]
    }
   ],
   "source": [
    "user_query_4 = \"What are the environmental impacts of aluminum production?\"\n",
    "chatbot(user_query_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ Query: What is red mud?\n",
      "\n",
      "🤖 Answer: Bauxite residue, containing caustic, Fe/Ti oxides. Smelting produces spent pot-linings and alumina-saturated slags. India is classifying red mud as hazardous waste to enforce safe disposal. Use-Phase (Lifetimes): Al products vary: e.g. foil/can packaging is short-lived (years), transportation and building components are long-lived (cars 10–20 yr; infrastru\n"
     ]
    }
   ],
   "source": [
    "user_query_3 = \"What is red mud?\"\n",
    "chatbot(user_query_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ Query: How much energy does recycling aluminum save?\n",
      "\n",
      "🤖 Answer: In India, aluminium smelters emit 20.9 tCO2 per tonne due to coal-based power. By contrast, recycling aluminum saves 95% of that energy. Recycling uses 5% of primary energy. Recycling rates: India 30–35%, global 65–75%. Typical waste: red mud (1–2 t per t alumina) and spent pot linings. Product lifetimes vary: packaging (years), transport/ construction (decades).\",\"metadata\":\"title\":\"Aluminium Productio Recycled Cu uses far less energy; ICA cites 85% energy savings vs primary.) Emissions: Copper smelters emit substantial pollutants: SO2 (if not fully captured), CO2 (from fuel, charcoal, and power), particulates (fugitive dust), and heavy metals (As, Pb, Cd) in slag/dust. Converting blister Cu is exothermic and produces CO2 from coke. LCA studies note that SO2 released (if not recovered)\n"
     ]
    }
   ],
   "source": [
    "user_query_2 = \"How much energy does recycling aluminum save?\"\n",
    "chatbot(user_query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ Query: What are the main production stages of Aluminium?\n",
      "\n",
      "🤖 Answer: Bauxite mining, alumina refining, and electrolysis are the main production stages of Aluminium. Typically “1 tonne of aluminium ingot (cradle-to-gate)” is used as the functional unit. Ensure the system boundary (mining to gate, etc.) is clearly defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_query = \"What are the main production stages of Aluminium?\"\n",
    "chatbot(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ Query: PRESIDENT OF INDIA ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Answer: The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India. The President of India is called the President of India.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"PRESIDENT OF INDIA ?\"\n",
    "chatbot(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
